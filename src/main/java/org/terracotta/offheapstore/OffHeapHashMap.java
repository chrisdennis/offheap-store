/* 
 * Copyright 2015 Terracotta, Inc., a Software AG company.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.terracotta.offheapstore;

import java.nio.ByteBuffer;
import java.nio.IntBuffer;
import java.util.AbstractMap;
import java.util.AbstractSet;
import java.util.ConcurrentModificationException;
import java.util.Iterator;
import java.util.Map;
import java.util.NoSuchElementException;
import java.util.Set;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.terracotta.offheapstore.buffersource.BufferSource;
import org.terracotta.offheapstore.exceptions.OversizeMappingException;
import org.terracotta.offheapstore.hashcode.HashCodeAlgorithm;
import org.terracotta.offheapstore.hashcode.JdkHashCodeAlgorithm;
import org.terracotta.offheapstore.paging.Page;
import org.terracotta.offheapstore.paging.PageSource;
import org.terracotta.offheapstore.storage.BinaryStorageEngine;
import org.terracotta.offheapstore.storage.StorageEngine;
import org.terracotta.offheapstore.util.DebuggingUtils;
import org.terracotta.offheapstore.util.FindbugsSuppressWarnings;
import org.terracotta.offheapstore.util.NoOpLock;
import org.terracotta.offheapstore.util.WeakIdentityHashMap;
import org.terracotta.offheapstore.util.WeakIdentityHashMap.ReaperTask;

import java.util.concurrent.locks.Lock;

import static java.lang.Long.rotateLeft;

/**
 * A hash-table implementation whose table is stored in an NIO direct buffer.
 * <p>
 * The map stores keys and values encoded as integers, in an open-addressed
 * linear-reprobing hashtable.  Entries are 16-bytes wide, and consist of:
 * <ul>
 *   <li><code>int</code> status marker</li>
 *   <li><code>int</code> cached key hashcode</li>
 *   <li><code>long</code> {key:value} representation</li>
 * </ul>
 * Key and value representations are generated by the {@link StorageEngine}
 * instance provided at construction time.
 * <p>
 * This {@link Map} implementation is not thread-safe and does not support null
 * keys or values.
 *
 * @param <K> the type of keys maintained by this map
 * @param <V> the type of mapped values
 *
 * @author Chris Dennis
 */
public class OffHeapHashMap<K, V> extends AbstractMap<K, V> implements MapInternals, StorageEngine.Owner {

  /*
   * Future design ideas:
   *
   * We might want to look in to reading the whole (or large chunks) of the
   * probe sequence for a key in one shot rather than doing it one by one.
   */

  private static final Logger LOGGER = LoggerFactory.getLogger(OffHeapHashMap.class);

  private static final int INITIAL_TABLE_SIZE = 128;
  private static final float TABLE_RESIZE_THRESHOLD = 0.5f;
  private static final float TABLE_SHRINK_THRESHOLD = 0.2f;
  private static final int INITIAL_REPROBE_LENGTH = 16;
  private static final int REPROBE_WARNING_THRESHOLD = 1024;
  private static final int ALLOCATE_ON_CLEAR_THRESHOLD_RATIO = 2;

  private static final IntBuffer DESTROYED_TABLE = IntBuffer.allocate(0);
  
  /**
   * Size of a table entry in primitive {@code int} units
   */
  protected static final int ENTRY_SIZE = 4;
  private static final int ENTRY_BIT_SHIFT = Integer.numberOfTrailingZeros(ENTRY_SIZE);

  protected static final int STATUS = 0;
  private static final int KEY_HASHCODE_LO = 1;
  private static final int ENCODING = 2;

  protected static final int STATUS_USED = 1;
  private static final int STATUS_REMOVED = 2;
  protected static final int STATUS_KEY_HASHCODE_HI = 0xffff0000;
  public static final int RESERVED_STATUS_BITS = STATUS_USED | STATUS_REMOVED | STATUS_KEY_HASHCODE_HI;

  protected final StorageEngine<? super K, ? super V> storageEngine;

  protected final PageSource tableSource;

  private final WeakIdentityHashMap<IntBuffer, PendingPage> pendingTableFrees = new WeakIdentityHashMap<IntBuffer, PendingPage>(new ReaperTask<PendingPage>() {
    @Override
    public void reap(PendingPage pending) {
      freeTable(pending.tablePage);
    }
  });

  private final int initialTableSize;

  private final boolean tableAllocationsSteal;

  private final ThreadLocal<Boolean> tableResizing = new ThreadLocal<Boolean>() {

    @Override
    protected Boolean initialValue() {
      return Boolean.FALSE;
    };
  };

  private final HashCodeAlgorithm hashing = new JdkHashCodeAlgorithm();

  protected volatile int size;

  protected volatile int modCount;

  /*
   * The reprobe limit as it currently stands only ever increases.  If we change
   * this behavior we will need to make changes to the iterators as they assume
   * this to be true.
   */
  protected int reprobeLimit = INITIAL_REPROBE_LENGTH;

  private float currentTableShrinkThreshold = TABLE_SHRINK_THRESHOLD;

  private volatile boolean hasUsedIterators;

  /**
   * The current hash-table.
   * <p>
   * A list of: {@code int[] {status, hashCode, encoding-high, encoding-low}}
   */
  protected volatile IntBuffer hashtable;
  protected volatile Page hashTablePage;

  private Set<Entry<K, V>> entrySet;
  private Set<K> keySet;
  private Set<Long> encodingSet;

  /*
   * Statistics support
   */
  protected volatile int removedSlots;

  /**
   * Construct an instance using a custom {@link BufferSource} for the
   * hashtable.
   *
   * @param source source for the hashtable allocations
   * @param storageEngine engine used to encode the keys and values
   */
  public OffHeapHashMap(PageSource source, StorageEngine<? super K, ? super V> storageEngine) {
    this(source, storageEngine, INITIAL_TABLE_SIZE);
  }

  public OffHeapHashMap(PageSource source, boolean tableAllocationsSteal, StorageEngine<? super K, ? super V> storageEngine) {
    this(source, tableAllocationsSteal, storageEngine, INITIAL_TABLE_SIZE);
  }

  public OffHeapHashMap(PageSource source, StorageEngine<? super K, ? super V> storageEngine, boolean bootstrap) {
    this(source, false, storageEngine, INITIAL_TABLE_SIZE, bootstrap);
  }

  /**
   * Construct an instance using a custom {@link BufferSource} for the
   * hashtable and a custom initial table size.
   *
   * @param source source for the hashtable allocations
   * @param storageEngine engine used to encode the keys and values
   * @param tableSize the initial table size
   */
  public OffHeapHashMap(PageSource source, StorageEngine<? super K, ? super V> storageEngine, int tableSize) {
    this(source, false, storageEngine, tableSize, true);
  }

  public OffHeapHashMap(PageSource source, boolean tableAllocationsSteal, StorageEngine<? super K, ? super V> storageEngine, int tableSize) {
    this(source, tableAllocationsSteal, storageEngine, tableSize, true);
  }

  @FindbugsSuppressWarnings("ICAST_INTEGER_MULTIPLY_CAST_TO_LONG")
  protected OffHeapHashMap(PageSource source, boolean tableAllocationsSteal, StorageEngine<? super K, ? super V> storageEngine, int tableSize, boolean bootstrap) {
    if (storageEngine == null) {
      throw new NullPointerException("StorageEngine implementation must be non-null");
    }

    this.storageEngine = storageEngine;
    this.tableSource = source;
    this.tableAllocationsSteal = tableAllocationsSteal;

    // Find a power of 2 >= initialCapacity
    int capacity = 1;
    while (capacity < tableSize) {
        capacity <<= 1;
    }
    this.initialTableSize = capacity;

    if (bootstrap) {
      this.hashTablePage = allocateTable(initialTableSize);
      if (hashTablePage == null) {
        StringBuilder sb = new StringBuilder("Initial table allocation failed.\n");
        sb.append("Initial Table Size (slots) : ").append(initialTableSize).append('\n');
        sb.append("Allocation Will Require    : ").append(DebuggingUtils.toBase2SuffixedString(initialTableSize * ENTRY_SIZE * (Integer.SIZE / Byte.SIZE))).append("B\n");
        sb.append("Table Page Source        : ").append(tableSource);
        throw new IllegalArgumentException(sb.toString());
      }
      hashtable = hashTablePage.asIntBuffer();
    }
    this.storageEngine.bind(this);
  }

  @Override
  public int size() {
    return size;
  }

  @Override
  public boolean containsKey(Object key) {
    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);

    if (size == 0) {
      return false;
    }
    
    IntBuffer view = (IntBuffer) hashtable.duplicate().position(indexFor(spread(maskHash)));

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!view.hasRemaining()) {
        view.rewind();
      }

      IntBuffer entry = (IntBuffer) view.slice().limit(ENTRY_SIZE);

      if (isTerminating(entry)) {
        return false;
      } else if (isPresent(entry) && keyEquals(key, maskHash, readEncoding(entry), readHash(entry))) {
        hit(entry);
        return true;
      } else {
        view.position(view.position() + ENTRY_SIZE);
      }
    }
    return false;
  }

  @SuppressWarnings("unchecked")
  @Override
  public V get(Object key) {
    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);

    if (size == 0) {
      return null;
    }
    
    IntBuffer view = (IntBuffer) hashtable.duplicate().position(indexFor(spread(maskHash)));

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!view.hasRemaining()) {
        view.rewind();
      }

      IntBuffer entry = (IntBuffer) view.slice().limit(ENTRY_SIZE);

      if (isTerminating(entry)) {
        return null;
      } else if (isPresent(entry) && keyEquals(key, maskHash, readEncoding(entry), readHash(entry))) {
        hit(entry);
        return (V) storageEngine.readValue(readEncoding(entry));
      } else {
        view.position(view.position() + ENTRY_SIZE);
      }
    }
    return null;
  }

  @Override
  public Long getEncodingForHashAndBinary(long fullHash, ByteBuffer binaryKey) {
    if (size == 0) {
      return null;
    }
    long maskHash = maskHash(fullHash);
    
    IntBuffer view = (IntBuffer) hashtable.duplicate().position(indexFor(spread(maskHash)));

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!view.hasRemaining()) {
        view.rewind();
      }

      IntBuffer entry = (IntBuffer) view.slice().limit(ENTRY_SIZE);

      if (isTerminating(entry)) {
        return null;
      } else if (isPresent(entry) && binaryKeyEquals(binaryKey, maskHash, readEncoding(entry), readHash(entry))) {
        return readEncoding(entry);
      } else {
        view.position(view.position() + ENTRY_SIZE);
      }
    }
    return null;
  }
  
  @Override
  @FindbugsSuppressWarnings("VO_VOLATILE_INCREMENT")
  public long installMappingForHashAndEncoding(long fullHash, ByteBuffer offheapBinaryKey, ByteBuffer offheapBinaryValue, int metadata) {
    freePendingTables(); 

    int[] newEntry = installEntry(offheapBinaryKey, fullHash, offheapBinaryValue, metadata);

    long maskHash = maskHash(fullHash);
    int start = indexFor(spread(maskHash));
    hashtable.position(start);

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      if (isAvailable(entry)) {
        if (isRemoved(entry)) {
          removedSlots--;
        }
        entry.put(newEntry);
        slotAdded(entry);
        hit(entry);
        return readEncoding(newEntry);
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }

    storageEngine.freeMapping(readEncoding(newEntry), readHash(newEntry), false); //XXX: further contemplate the boolean value here

    // hit reprobe limit - must rehash
    expand(start, limit);
    
    return installMappingForHashAndEncoding(fullHash, offheapBinaryKey, offheapBinaryValue, metadata);
  }

  public Integer getMetadata(Object key, int mask) {
    int safeMask = mask & ~RESERVED_STATUS_BITS;
    
    freePendingTables();

    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);
    hashtable.position(indexFor(spread(maskHash)));

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      long encoding = readEncoding(entry);
      if (isTerminating(entry)) {
        return null;
      } else if (isPresent(entry) && keyEquals(key, maskHash, encoding, readHash(entry))) {
        return entry.get(STATUS) & safeMask;
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }

    return null;
  }

  public Integer getAndSetMetadata(Object key, int mask, int values) {
    int safeMask = mask & ~RESERVED_STATUS_BITS;
    
    freePendingTables();

    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);
    hashtable.position(indexFor(spread(maskHash)));

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      long encoding = readEncoding(entry);
      if (isTerminating(entry)) {
        return null;
      } else if (isPresent(entry) && keyEquals(key, maskHash, encoding, readHash(entry))) {
        int previous = entry.get(STATUS);
        entry.put(STATUS, (previous & ~safeMask) | (values & safeMask));
        return previous & safeMask;
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }

    return null;
  }

  public V getValueAndSetMetadata(Object key, int mask, int values) {
    int safeMask = mask & ~RESERVED_STATUS_BITS;
    
    freePendingTables();

    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);
    hashtable.position(indexFor(spread(maskHash)));

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      long encoding = readEncoding(entry);
      if (isTerminating(entry)) {
        return null;
      } else if (isPresent(entry) && keyEquals(key, maskHash, encoding, readHash(entry))) {
        hit(entry);
        entry.put(STATUS, (entry.get(STATUS) & ~safeMask) | (values & safeMask));
        return (V) storageEngine.readValue(readEncoding(entry));
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }

    return null;
  }

  @Override
  public V put(K key, V value) {
    return put(key, value, 0);
  }

  @SuppressWarnings("unchecked")
  @FindbugsSuppressWarnings("VO_VOLATILE_INCREMENT")
  public V put(K key, V value, int metadata) {
    freePendingTables();

    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);

    int[] newEntry = writeEntry(key, fullHash, value, metadata);

    int start = indexFor(spread(maskHash));
    hashtable.position(start);

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      if (isAvailable(entry)) {
        storageEngine.attachedMapping(readEncoding(newEntry), fullHash, metadata);
        storageEngine.invalidateCache();
        for (IntBuffer laterEntry = entry; i < limit; i++) {
          if (isTerminating(laterEntry)) {
            break;
          } else if (isPresent(laterEntry) && keyEquals(key, maskHash, readEncoding(laterEntry), readHash(laterEntry))) {
            V old = (V) storageEngine.readValue(readEncoding(laterEntry));
            storageEngine.freeMapping(readEncoding(laterEntry), readHash(laterEntry), false);
            long oldEncoding = readEncoding(laterEntry);
            laterEntry.put(newEntry);
            slotUpdated((IntBuffer) laterEntry.flip(), oldEncoding);
            hit(laterEntry);
            return old;
          } else {
            hashtable.position(hashtable.position() + ENTRY_SIZE);
          }

          if (!hashtable.hasRemaining()) {
            hashtable.rewind();
          }

          laterEntry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);
        }
        if (isRemoved(entry)) {
          removedSlots--;
        }
        entry.put(newEntry);
        slotAdded(entry);
        hit(entry);
        return null;
      } else if (keyEquals(key, maskHash, readEncoding(entry), readHash(entry))) {
        storageEngine.attachedMapping(readEncoding(newEntry), fullHash, metadata);
        storageEngine.invalidateCache();
        V old = (V) storageEngine.readValue(readEncoding(entry));
        storageEngine.freeMapping(readEncoding(entry), readHash(entry), false);
        long oldEncoding = readEncoding(entry);
        entry.put(newEntry);
        slotUpdated((IntBuffer) entry.flip(), oldEncoding);
        hit(entry);
        return old;
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }

    storageEngine.freeMapping(readEncoding(newEntry), readHash(newEntry), false); //XXX: further contemplate the boolean value here

    // hit reprobe limit - must rehash
    expand(start, limit);

    return put(key, value, metadata);
  }

  /**
   * Associates the specified value with the specified key in this map.  If the
   * map does not contain a mapping for the key, the new mapping is only
   * installed if there is room.  If the map previously contained a mapping for
   * the key, the old value is replaced by the specified value even if this
   * results in a failure or eviction.
   *
   * @param key key with which the specified value is to be associated
   * @param value value to be associated with the specified key
   * @return the previous value associated with <tt>key</tt>, or
   *         <tt>null</tt> if there was no mapping for <tt>key</tt>
   *         (irrespective of whether the value was successfully installed).
   */
  public V fill(K key, V value) {
    return fill(key, value, 0);
  }

  @FindbugsSuppressWarnings("VO_VOLATILE_INCREMENT")
  public V fill(K key, V value, int metadata) {
    freePendingTables();

    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);

    int start = indexFor(spread(maskHash));
    hashtable.position(start);

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      if (isAvailable(entry)) {
        for (IntBuffer laterEntry = entry; i < limit; i++) {
          if (isTerminating(laterEntry)) {
            break;
          } else if (isPresent(laterEntry) && keyEquals(key, maskHash, readEncoding(laterEntry), readHash(laterEntry))) {
            return put(key, value, metadata);
          } else {
            hashtable.position(hashtable.position() + ENTRY_SIZE);
          }

          if (!hashtable.hasRemaining()) {
            hashtable.rewind();
          }

          laterEntry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);
        }

        int[] newEntry = tryWriteEntry(key, maskHash, value, metadata);
        if (newEntry == null) {
          return null;
        } else {
          return fill(key, value, fullHash, newEntry, metadata);
        }
      } else if (keyEquals(key, maskHash, readEncoding(entry), readHash(entry))) {
        return put(key, value, metadata);
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }

    // hit reprobe limit - must rehash
    if (tryExpandTable()) {
      return fill(key, value, metadata);
    } else {
      return null;
    }
  }

  @SuppressWarnings("unchecked")
  @FindbugsSuppressWarnings("VO_VOLATILE_INCREMENT")
  protected final V fill(K key, V value, long fullHash, int[] newEntry, int metadata) {
    freePendingTables();

    long maskHash = maskHash(fullHash);
    int start = indexFor(spread(maskHash));
    hashtable.position(start);

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      if (isAvailable(entry)) {
        storageEngine.attachedMapping(readEncoding(newEntry), fullHash, metadata);
        storageEngine.invalidateCache();
        for (IntBuffer laterEntry = entry; i < limit; i++) {
          if (isTerminating(laterEntry)) {
            break;
          } else if (isPresent(laterEntry) && keyEquals(key, maskHash, readEncoding(laterEntry), readHash(laterEntry))) {
            V old = (V) storageEngine.readValue(readEncoding(laterEntry));
            storageEngine.freeMapping(readEncoding(laterEntry), readHash(laterEntry), false);
            long oldEncoding = readEncoding(laterEntry);
            laterEntry.put(newEntry);
            slotUpdated((IntBuffer) laterEntry.flip(), oldEncoding);
            hit(laterEntry);
            return old;
          } else {
            hashtable.position(hashtable.position() + ENTRY_SIZE);
          }

          if (!hashtable.hasRemaining()) {
            hashtable.rewind();
          }

          laterEntry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);
        }
        if (isRemoved(entry)) {
          removedSlots--;
        }
        entry.put(newEntry);
        slotAdded(entry);
        hit(entry);
        return null;
      } else if (keyEquals(key, maskHash, readEncoding(entry), readHash(entry))) {
        storageEngine.attachedMapping(readEncoding(newEntry), fullHash, metadata);
        storageEngine.invalidateCache();
        V old = (V) storageEngine.readValue(readEncoding(entry));
        storageEngine.freeMapping(readEncoding(entry), readHash(entry), false);
        long oldEncoding = readEncoding(entry);
        entry.put(newEntry);
        slotUpdated((IntBuffer) entry.flip(), oldEncoding);
        hit(entry);
        return old;
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }

    storageEngine.freeMapping(readEncoding(newEntry), readHash(newEntry), true);

    return null;
  }

  private int[] writeEntry(K key, long fullHash, V value, int metadata) {
    /*
     * TODO If we start supporting remapping (compaction) then we'll need to
     * worry about correcting the key representation here if the value write
     * triggers a remapping operation.
     */
    while (true) {
      int[] entry = tryWriteEntry(key, fullHash, value, metadata);
      if (entry == null) {
        storageEngineFailure(key);
      } else {
        return entry;
      }
    }
  }

  @FindbugsSuppressWarnings("PZLA_PREFER_ZERO_LENGTH_ARRAYS")
  private int[] tryWriteEntry(K key, long fullHash, V value, int metadata) {
    if (hashtable == null) {
      throw new NullPointerException();
    } else if (hashtable == DESTROYED_TABLE) {
      throw new IllegalStateException("Offheap map/cache has been destroyed");
    } else if ((metadata & RESERVED_STATUS_BITS) == 0) {
      Long encoding = storageEngine.writeMapping(key, value, fullHash, metadata);
      if (encoding == null) {
        return null;
      } else {
        return createEntry(maskHash(fullHash), encoding, metadata);
      }
    } else {
      throw new IllegalArgumentException("Invalid metadata for key '" + key + "' : " + Integer.toBinaryString(metadata));
    }
  }

  private int[] installEntry(ByteBuffer offheapBinaryKey, long fullHash, ByteBuffer offheapBinaryValue, int metadata) {
    while (true) {
      int [] entry = tryInstallEntry(offheapBinaryKey, fullHash, offheapBinaryValue, metadata);
      if (entry == null) {
        storageEngineFailure("<binary-key>");
        continue;
      } else {
        return entry;
      }
    }
  }
  
  @FindbugsSuppressWarnings("PZLA_PREFER_ZERO_LENGTH_ARRAYS")
  private int[] tryInstallEntry(ByteBuffer offheapBinaryKey, long fullHash, ByteBuffer offheapBinaryValue, int metadata) {
    if (hashtable == null) {
      throw new NullPointerException();
    } else if (hashtable == DESTROYED_TABLE) {
      throw new IllegalStateException("Offheap map/cache has been destroyed");
    } else if ((metadata & RESERVED_STATUS_BITS) == 0) {
      Long encoding = ((BinaryStorageEngine) storageEngine).writeBinaryMapping(offheapBinaryKey, offheapBinaryValue, fullHash, metadata);
      if (encoding == null) {
        return null;
      } else {
        return createEntry(maskHash(fullHash), encoding, metadata);
      }
    } else {
      throw new IllegalArgumentException("Invalid metadata for binary key : " + Integer.toBinaryString(metadata));
    }
  }
  
  private static int[] createEntry(long maskHash, long encoding, int metadata) {
    int status = STATUS_USED | metadata | (int) ((maskHash >>> 16) & STATUS_KEY_HASHCODE_HI);
    return new int[] { status, (int) maskHash, (int) (encoding >>> Integer.SIZE), (int) encoding };
  }

  private static long maskHash(long hash) {
    return hash & 0x0000ffffffffffffL;
  }

  @Override
  public V remove(Object key) {
    freePendingTables();

    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);

    if (size == 0) {
      return null;
    }
    
    hashtable.position(indexFor(spread(maskHash)));

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      if (isTerminating(entry)) {
        return null;
      } else if (isPresent(entry) && keyEquals(key, maskHash, readEncoding(entry), readHash(entry))) {
        @SuppressWarnings("unchecked")
        V removedValue = (V) storageEngine.readValue(readEncoding(entry));
        storageEngine.freeMapping(readEncoding(entry), readHash(entry), true);

        /*
         * TODO We might want to track the number of 'removed' slots in the
         * table, and rehash it if we reach some threshold to avoid lookup costs
         * staying artificially high when the table is relatively empty, but full
         * of 'removed' slots.  This situation should be relatively rare for
         * normal cache usage - but might be more common for more map like usage
         * patterns.
         *
         * The more severe versions of this pattern are now handled by the table
         * shrinking when the occupation drops below the shrink threshold, as
         * that will rehash the table.
         */
        entry.put(STATUS, STATUS_REMOVED);
        slotRemoved(entry);
        shrink();
        return removedValue;
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }

    return null;
  }

  public boolean removeNoReturn(Object key) {
    freePendingTables();

    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);

    hashtable.position(indexFor(spread(maskHash)));

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      if (isTerminating(entry)) {
        return false;
      } else if (isPresent(entry) && keyEquals(key, maskHash, readEncoding(entry), readHash(entry))) {
        storageEngine.freeMapping(readEncoding(entry), readHash(entry), true);

        /*
         * TODO We might want to track the number of 'removed' slots in the
         * table, and rehash it if we reach some threshold to avoid lookup costs
         * staying artificially high when the table is relatively empty, but full
         * of 'removed' slots.  This situation should be relatively rare for
         * normal cache usage - but might be more common for more map like usage
         * patterns.
         *
         * The more severe versions of this pattern are now handled by the table
         * shrinking when the occupation drops below the shrink threshold, as
         * that will rehash the table.
         */
        entry.put(STATUS_REMOVED);
        slotRemoved(entry);
        shrink();
        return true;
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }
    return false;
  }
  
  @Override
  @FindbugsSuppressWarnings("VO_VOLATILE_INCREMENT")
  public void clear() {
    if (hashtable != DESTROYED_TABLE) {
      freePendingTables();

      modCount++;
      removedSlots = 0;
      size = 0;
      storageEngine.clear();
      allocateOrClearTable(initialTableSize);
    }
  }

  public void destroy() {
    removedSlots = 0;
    size = 0;
    freeTable(hashTablePage);
    for (Iterator<PendingPage> it = pendingTableFrees.values(); it.hasNext(); freeTable(it.next().tablePage));
    hashTablePage = null;
    hashtable = DESTROYED_TABLE;
    storageEngine.destroy();
  }

  private void allocateOrClearTable(int size) {
    int[] zeros = new int[1024 >> 2];
    hashtable.clear();
    while (hashtable.hasRemaining()) {
      if (hashtable.remaining() < zeros.length) {
        hashtable.put(zeros, 0, hashtable.remaining());
      } else {
        hashtable.put(zeros);
      }
    }
    hashtable.clear();
    
    wipePendingTables();
    
    if (hashtable.capacity() > size * ENTRY_SIZE * ALLOCATE_ON_CLEAR_THRESHOLD_RATIO) {
      Page newTablePage = allocateTable(size);
      if (newTablePage != null) {
        freeTable(hashTablePage, hashtable, reprobeLimit());
        hashTablePage = newTablePage;
        hashtable = newTablePage.asIntBuffer();
      }
    }
  }

  @Override
  public Set<Entry<K, V>> entrySet() {
    Set<Entry<K, V>> es = entrySet;
    return es == null ? (entrySet = new EntrySet()) : es;
  }

  @Override
  public Set<Long> encodingSet() {
    Set<Long> es = encodingSet;
    return es == null ? (encodingSet = new EncodingSet()) : es;
  }

  @Override
  public Set<K> keySet() {
    Set<K> ks = keySet;
    return ks == null ? (keySet = new KeySet()) : ks;
  }

  protected static boolean isPresent(IntBuffer entry) {
    return (entry.get(STATUS) & STATUS_USED) != 0;
  }

  protected static boolean isAvailable(IntBuffer entry) {
    return (entry.get(STATUS) & STATUS_USED) == 0;
  }

  protected static boolean isTerminating(IntBuffer entry) {
    return isTerminating(entry.get(STATUS));
  }

  private static boolean isTerminating(int entryStatus) {
    return (entryStatus & (STATUS_USED | STATUS_REMOVED)) == 0;
  }

  protected static boolean isRemoved(IntBuffer entry) {
    return isRemoved(entry.get(STATUS));
  }

  private static boolean isRemoved(int entryStatus) {
    return (entryStatus & STATUS_REMOVED) != 0;
  }

  private static long readHash(IntBuffer entry) {
    return ((0xffffffffL & STATUS_KEY_HASHCODE_HI & entry.get(STATUS)) << 16) | (0xffffffffL & entry.get(KEY_HASHCODE_LO));
  }
  
  private static long readHash(int[] entry) {
    return ((0xffffffffL & STATUS_KEY_HASHCODE_HI & entry[STATUS]) << 16) | (0xffffffffL & entry[KEY_HASHCODE_LO]);
  }
  
  private static long readEncoding(IntBuffer entry) {
    return (((long) entry.get(ENCODING)) << Integer.SIZE) | (0xffffffffL & entry.get(ENCODING + 1));
  }
  
  private static long readEncoding(int[] entry) {
    return (((long) entry[ENCODING]) << Integer.SIZE) | (0xffffffffL & entry[ENCODING + 1]);
  }

  private int indexFor(long maskHash) {
    return indexFor(maskHash, hashtable);
  }

  private static int indexFor(long maskHash, IntBuffer table) {
    return (int) ((maskHash << ENTRY_BIT_SHIFT) & Math.max(0, table.capacity() - 1));
  }

  private boolean keyEquals(Object probeKey, long probeHash, long targetEncoding, long targetHash) {
    return probeHash == targetHash && storageEngine.equalsKey(probeKey, targetEncoding);
  }

  private boolean binaryKeyEquals(ByteBuffer binaryProbeKey, long probeHash, long targetEncoding, long targetHash) {
    if (storageEngine instanceof BinaryStorageEngine) {
      return probeHash == targetHash && ((BinaryStorageEngine) storageEngine).equalsBinaryKey(binaryProbeKey, targetEncoding);
    } else {
      throw new UnsupportedOperationException("Cannot check binary quality unless configured with a BinaryStorageEngine");
    }
  }
  
  private void expand(int start, int length) {
    if (!tryExpand()) {
      tableExpansionFailure(start, length);
    }
  }

  private boolean tryExpand() {
    if (((float) size) / getTableCapacity() > TABLE_RESIZE_THRESHOLD) {
      return tryExpandTable();
    } else {
      return tryIncreaseReprobe();
    }
  }

  private boolean tryExpandTable() {
    if (tableResizing.get()) {
      throw new AssertionError("Expand requested in context of an existing resize - this should be impossible");
    } else {
      tableResizing.set(Boolean.TRUE);
      try {
        Page newTablePage = expandTable(1);
        if (newTablePage == null) {
          return false;
        } else {
          freeTable(hashTablePage, hashtable, reprobeLimit());
          hashTablePage = newTablePage;
          hashtable = newTablePage.asIntBuffer();
          removedSlots = 0;
          return true;
        }
      } finally {
        tableResizing.remove();
      }
    }
  }

  private Page expandTable(int scale) {
    if (hashtable == DESTROYED_TABLE) {
      throw new IllegalStateException("This map/cache has been destroyed");
    }
    
    /* Increase the size of the table to accommodate more entries */
    int newsize = hashtable.capacity() << scale;

    /* Check we're not hitting max capacity */
    if (newsize <= 0) {
      return null;
    }

    long startTime = -1;

    if (LOGGER.isDebugEnabled()) {
      startTime = System.nanoTime();
      int slots = hashtable.capacity() / ENTRY_SIZE;
      int newslots = newsize / ENTRY_SIZE;
      LOGGER.debug("Expanding table from {} slots to {} slots [load-factor={}]",
              new Object[] {DebuggingUtils.toBase2SuffixedString(slots),
                            DebuggingUtils.toBase2SuffixedString(newslots),
                            ((float) size) / slots});
    }

    Page newTablePage = allocateTable(newsize / ENTRY_SIZE);
    if (newTablePage == null) {
      return null;
    }

    IntBuffer newTable = newTablePage.asIntBuffer();

    for (hashtable.clear(); hashtable.hasRemaining(); hashtable.position(hashtable.position() + ENTRY_SIZE)) {
      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      if (isPresent(entry) && !writeEntry(newTable, entry)) {
        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug("Table expansion from {} slots to {} slots abandoned - not enough table space",
                  DebuggingUtils.toBase2SuffixedString(hashtable.capacity() / ENTRY_SIZE),
                  DebuggingUtils.toBase2SuffixedString(newsize / ENTRY_SIZE));
        }
        freeTable(newTablePage);
        return expandTable(scale + 1);
      }
    }

    if (LOGGER.isDebugEnabled()) {
      long time = System.nanoTime() - startTime;
      LOGGER.debug("Table expansion from {} slots to {} slots complete : took {}ms", new Object[] {
              DebuggingUtils.toBase2SuffixedString(hashtable.capacity() / ENTRY_SIZE),
              DebuggingUtils.toBase2SuffixedString(newsize / ENTRY_SIZE),
              ((float) time) / 1000000});
    }

    return newTablePage;
  }

  protected boolean tryIncreaseReprobe() {
    if (reprobeLimit() >= getTableCapacity()) {
      return false;
    } else {
      int newReprobeLimit = reprobeLimit() << 1;

      if (newReprobeLimit >= REPROBE_WARNING_THRESHOLD) {
        long slots = getTableCapacity();
        LOGGER.warn("Expanding reprobe sequence from {} slots to {} slots [load-factor={}]",
                new Object[] {reprobeLimit(), newReprobeLimit, ((float) size) / slots});
      } else if (LOGGER.isDebugEnabled()) {
        long slots = getTableCapacity();
        LOGGER.debug("Expanding reprobe sequence from {} slots to {} slots [load-factor={}]",
                new Object[] {reprobeLimit(), newReprobeLimit, ((float) size) / slots});
      }

      reprobeLimit = newReprobeLimit;
      return true;
    }
  }

  private void shrink() {
    if (((float) size) / getTableCapacity() <= currentTableShrinkThreshold) {
      shrinkTable();
    }
  }

  private void shrinkTable() {
    if (tableResizing.get()) {
      LOGGER.debug("Shrink request ignored in the context of an in-process expand - likely self stealing");
    } else {
      tableResizing.set(Boolean.TRUE);
      try {
        float shrinkRatio = (TABLE_RESIZE_THRESHOLD * getTableCapacity()) / size;
        int shrinkShift = Integer.numberOfTrailingZeros(Integer.highestOneBit(Math.max(2, (int) shrinkRatio)));
        Page newTablePage = shrinkTable(shrinkShift);
        if (newTablePage == null) {
          currentTableShrinkThreshold = currentTableShrinkThreshold / 2;
        } else {
          currentTableShrinkThreshold = TABLE_SHRINK_THRESHOLD;
          freeTable(hashTablePage, hashtable, reprobeLimit());
          hashTablePage = newTablePage;
          hashtable = newTablePage.asIntBuffer();
          removedSlots = 0;
        }
      } finally {
        tableResizing.remove();
      }
    }
  }

  private Page shrinkTable(int scale) {
    /* Increase the size of the table to accommodate more entries */
    int newsize = hashtable.capacity() >>> scale;

    /* Check we're not hitting zero capacity */
    if (newsize < ENTRY_SIZE) {
      if (scale > 1) {
        return shrinkTable(scale - 1);
      } else {
        return null;
      }
    }

    long startTime = -1;

    if (LOGGER.isDebugEnabled()) {
      startTime = System.nanoTime();
      int slots = hashtable.capacity() / ENTRY_SIZE;
      int newslots = newsize / ENTRY_SIZE;
      LOGGER.debug("Shrinking table from {} slots to {} slots [load-factor={}]",
              new Object[] {DebuggingUtils.toBase2SuffixedString(slots),
              DebuggingUtils.toBase2SuffixedString(newslots),
              ((float) size) / slots});
    }

    Page newTablePage = allocateTable(newsize / ENTRY_SIZE);
    if (newTablePage == null) {
      return null;
    }

    IntBuffer newTable = newTablePage.asIntBuffer();
    
    for (hashtable.clear(); hashtable.hasRemaining(); hashtable.position(hashtable.position() + ENTRY_SIZE)) {
      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      if (isPresent(entry) && !writeEntry(newTable, entry)) {
        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug("Table shrinking from {} slots to {} slots abandoned - too little table space",
                  DebuggingUtils.toBase2SuffixedString(hashtable.capacity() / ENTRY_SIZE),
                  DebuggingUtils.toBase2SuffixedString(newsize / ENTRY_SIZE));
        }
        freeTable(newTablePage);
        if (scale > 1) {
          return shrinkTable(scale - 1);
        } else {
          hashtable.clear();
          return null;
        }
      }
    }

    if (LOGGER.isDebugEnabled()) {
      long time = System.nanoTime() - startTime;
      LOGGER.debug("Table shrinking from {} slots to {} slots complete : took {}ms", new Object[] {
              DebuggingUtils.toBase2SuffixedString(hashtable.capacity() / ENTRY_SIZE),
              DebuggingUtils.toBase2SuffixedString(newsize / ENTRY_SIZE),
              ((float) time) / 1000000});
    }

    return newTablePage;
  }

  private boolean writeEntry(IntBuffer table, IntBuffer entry) {
    int start = indexFor(spread(readHash(entry)), table);
    int tableMask = table.capacity() - 1;

    for (int i = 0; i < reprobeLimit() * ENTRY_SIZE; i += ENTRY_SIZE) {
      int address = (start + i) & tableMask;
      int existingStatus = table.get(address + STATUS);
      if (isTerminating(existingStatus)) {
        table.position(address);
        table.put(entry);
        return true;
      } else if (isRemoved(existingStatus)) {
        throw new AssertionError();
      }
    }

    return false;
  }

  private static long spread(long hash) {
    return hashlittle2(hash, 0L);
  }

  private static long hashlittle2(long hash, long seed) {
    int a = 0xdeadbeef + 8 + ((int) seed) + ((int) hash);
    int b = 0xdeadbeef + 8 + ((int) seed) + ((int) (hash >>> Integer.SIZE));
    int c = 0xdeadbeef + 8 + ((int) seed) + ((int) (seed >>> Integer.SIZE));

    c ^= b; c -= rotateLeft(b, 14);
    a ^= c; a -= rotateLeft(c,11);
    b ^= a; b -= rotateLeft(a,25);
    c ^= b; c -= rotateLeft(b,16);
    a ^= c; a -= rotateLeft(c,4);
    b ^= a; b -= rotateLeft(a,14);
    c ^= b; c -= rotateLeft(b,24);
    
    return (0xffffffffL & c) | (((long) b) << Integer.SIZE);
  }

  private Page allocateTable(int size) {
    Page newTablePage = tableSource.allocate(size * ENTRY_SIZE * (Integer.SIZE / Byte.SIZE), tableAllocationsSteal, false, null);
    if (newTablePage != null) {
      ByteBuffer buffer = newTablePage.asByteBuffer();
      byte[] zeros = new byte[1024];
      buffer.clear();
      while (buffer.hasRemaining()) {
        if (buffer.remaining() < zeros.length) {
          buffer.put(zeros, 0, buffer.remaining());
        } else {
          buffer.put(zeros);
        }
      }
      buffer.clear();
    }
    return newTablePage;
  }

  private void freeTable(Page tablePage, IntBuffer table, int finalReprobe) {
    if (hasUsedIterators) {
      pendingTableFrees.put(table, new PendingPage(tablePage, finalReprobe));
    } else {
      freeTable(tablePage);
    }
  }

  private void freeTable(Page tablePage) {
    tableSource.free(tablePage);
  }

  private int reprobeLimit() {
    return reprobeLimit;
  }

  class EntrySet extends AbstractSet<Entry<K, V>> {

    @Override
    public Iterator<Map.Entry<K, V>> iterator() {
      return new EntryIterator();
    }

    @Override
    public boolean contains(Object o) {
      if (!(o instanceof Entry<?, ?>)) {
        return false;
      }
      Entry<?, ?> e = (Entry<?, ?>) o;
      V value = get(e.getKey());
      return value != null && value.equals(e.getValue());
    }

    @Override
    public boolean remove(Object o) {
      return removeMapping(o);
    }

    @Override
    public int size() {
      return size;
    }

    @Override
    public void clear() {
      OffHeapHashMap.this.clear();
    }
  }

  class EncodingSet extends AbstractSet<Long> {
    @Override
    public Iterator<Long> iterator() {
      return new EncodingIterator();
    }

    @Override
    public int size() {
      return size;
    }

    @Override
    public boolean contains(Object o) {
      // We could allow the impl from AbstractSet to run but that won't perform as well as you'd expect Set.contains() to run
      throw new UnsupportedOperationException();
    }
  }

  class KeySet extends AbstractSet<K> {

    @Override
    public Iterator<K> iterator() {
      return new KeyIterator();
    }

    @Override
    public boolean contains(Object o) {
      return OffHeapHashMap.this.containsKey(o);
    }

    @Override
    public boolean remove(Object o) {
      return OffHeapHashMap.this.remove(o) != null;
    }

    @Override
    public int size() {
      return OffHeapHashMap.this.size();
    }

    @Override
    public void clear() {
      OffHeapHashMap.this.clear();
    }
  }

  abstract class HashIterator<T> implements Iterator<T> {

    final int expectedModCount; // For fast-fail
    /*
     * We *must* keep a reference to the original table object that is the key
     * in the weak map to prevent the table from being freed
     */
    final IntBuffer table;
    final IntBuffer tableView;
    
    T next = null; // next entry to return

    HashIterator() {
      hasUsedIterators = true;
      table = hashtable;
      tableView = (IntBuffer) table.asReadOnlyBuffer().clear();
      expectedModCount = modCount;

      if (size > 0) { // advance to first entry
        while (tableView.hasRemaining()) {
          IntBuffer entry = (IntBuffer) tableView.slice().limit(ENTRY_SIZE);
          tableView.position(tableView.position() + ENTRY_SIZE);
          
          if (isPresent(entry)) {
            next = create(entry);
            break;
          }
        }
      }
    }

    protected abstract T create(IntBuffer entry);

    @Override
    public boolean hasNext() {
      return next != null;
    }

    @Override
    public T next() {
      checkForConcurrentModification();

      T e = next;
      if (e == null) {
        throw new NoSuchElementException();
      }

      next = null;
      while (tableView.hasRemaining()) {
        IntBuffer entry = (IntBuffer) tableView.slice().limit(ENTRY_SIZE);
        tableView.position(tableView.position() + ENTRY_SIZE);

        if (isPresent(entry)) {
          next = create(entry);
          break;
        }
      }
      return e;
    }

    @Override
    public void remove() {
      throw new UnsupportedOperationException();
    }

    protected void checkForConcurrentModification() {
      if (modCount != expectedModCount) {
        throw new ConcurrentModificationException();
      }
    }
  }

  static class PendingPage {
    final Page tablePage;
    final int reprobe;

    PendingPage(Page tablePage, int reprobe) {
      this.tablePage = tablePage;
      this.reprobe = reprobe;
    }
  }

  private void freePendingTables() {
    if (hasUsedIterators) {
      pendingTableFrees.reap();
    }
  }

  private void updatePendingTables(long fullHash, long oldEncoding, IntBuffer newEntry) {
    if (hasUsedIterators) {
      long maskHash = maskHash(fullHash);

      pendingTableFrees.reap();
      
      Iterator<PendingPage> it = pendingTableFrees.values();
      while (it.hasNext()) {
        PendingPage pending = it.next();
        
        IntBuffer pendingTable = pending.tablePage.asIntBuffer();
        pendingTable.position(indexFor(spread(maskHash), pendingTable));
        
        for (int i = 0; i < pending.reprobe; i++) {
          if (!pendingTable.hasRemaining()) {
            pendingTable.rewind();
          }
  
          IntBuffer entry = (IntBuffer) pendingTable.slice().limit(ENTRY_SIZE);
  
          if (isTerminating(entry)) {
            break;
          } else if (isPresent(entry) && (maskHash == readHash(entry)) && (oldEncoding == readEncoding(entry))) {
            entry.put(newEntry.duplicate());
            break;
          } else {
            pendingTable.position(pendingTable.position() + ENTRY_SIZE);
          }
        }
      }
    }
  }

  private void wipePendingTables() {
    if (hasUsedIterators) {
      pendingTableFrees.reap();
      
      int[] zeros = new int[1024 >> 2];
      
      Iterator<PendingPage> it = pendingTableFrees.values();
      while (it.hasNext()) {
        PendingPage pending = it.next();
        
        IntBuffer pendingTable = pending.tablePage.asIntBuffer();
        
        pendingTable.clear();
        while (pendingTable.hasRemaining()) {
          if (pendingTable.remaining() < zeros.length) {
            pendingTable.put(zeros, 0, pendingTable.remaining());
          } else {
            pendingTable.put(zeros);
          }
        }
        pendingTable.clear();
      }
    }
  }
  
  class KeyIterator extends HashIterator<K> {
    KeyIterator() {
      super();
    }

    @Override
    @SuppressWarnings("unchecked")
    protected K create(IntBuffer entry) {
      return (K) storageEngine.readKey(readEncoding(entry), readHash(entry));
    }

  }

  class EntryIterator extends HashIterator<Entry<K, V>> {
    EntryIterator() {
      super();
    }

    @Override
    protected Entry<K, V> create(IntBuffer entry) {
      return new DirectEntry(entry);
    }
  }

  class EncodingIterator extends HashIterator<Long> {
    EncodingIterator() {
      super();
    }

    @Override
    protected Long create(IntBuffer entry) {
      return readEncoding(entry);
    }
  }

  class DirectEntry implements Entry<K, V> {

    private final K key;
    private final V value;

    @SuppressWarnings("unchecked")
    DirectEntry(IntBuffer entry) {
      this.key = (K) storageEngine.readKey(readEncoding(entry), readHash(entry));
      this.value = (V) storageEngine.readValue(readEncoding(entry));
    }

    @Override
    public K getKey() {
      return key;
    }

    @Override
    public V getValue() {
      return value;
    }

    @Override
    public V setValue(V value) {
      throw new UnsupportedOperationException();
    }

    @Override
    public int hashCode() {
      return key.hashCode() ^ value.hashCode();
    }

    @Override
    public boolean equals(Object o) {
      if (o instanceof Entry<?, ?>) {
        Entry<?, ?> e = (Entry<?, ?>) o;
        return key.equals(e.getKey()) && value.equals(e.getValue());
      } else {
        return false;
      }
    }

    @Override
    public String toString() {
      return key + "=" + value;
    }
  }

  /*
   * remove used by EntrySet
   */
  @SuppressWarnings("unchecked")
  protected boolean removeMapping(Object o) {
    freePendingTables();

    if (!(o instanceof Entry<?, ?>)) {
      return false;
    }

    Entry<K, V> e = (Entry<K, V>) o;

    Object key = e.getKey();
    long fullHash = hashing.hash(key);
    long maskHash = maskHash(fullHash);

    hashtable.position(indexFor(spread(maskHash)));

    for (int i = 0; i < reprobeLimit(); i++) {
      if (!hashtable.hasRemaining()) {
        hashtable.rewind();
      }

      IntBuffer entry = (IntBuffer) hashtable.slice().limit(ENTRY_SIZE);

      if (isTerminating(entry)) {
        return false;
      } else if (isPresent(entry) && keyEquals(key, maskHash, readEncoding(entry), readHash(entry))
          && storageEngine.equalsValue(e.getValue(), readEncoding(entry))) {
        storageEngine.freeMapping(readEncoding(entry), readHash(entry), true);

        entry.put(STATUS_REMOVED);
        slotRemoved(entry);
        shrink();
        return true;
      } else {
        hashtable.position(hashtable.position() + ENTRY_SIZE);
      }
    }

    return false;
  }

  @Override
  public boolean evict(int index, boolean shrink) {
    return false;
  }

  protected void removeAtTableOffset(int offset, boolean shrink) {
    IntBuffer entry = ((IntBuffer) hashtable.duplicate().position(offset).limit(offset + ENTRY_SIZE)).slice();

    if (isPresent(entry)) {
      storageEngine.freeMapping(readEncoding(entry), readHash(entry), true);

      entry.put(STATUS, STATUS_REMOVED);
      slotRemoved(entry);
      if (shrink) {
        shrink();
      }
    } else {
      throw new AssertionError();
    }
  }

  @SuppressWarnings("unchecked")
  protected V getAtTableOffset(int offset) {
    IntBuffer entry = ((IntBuffer) hashtable.duplicate().position(offset).limit(offset + ENTRY_SIZE)).slice();

    if (isPresent(entry)) {
      return (V) storageEngine.readValue(readEncoding(entry));
    } else {
      throw new AssertionError();
    }
  }

  protected Entry<K, V> getEntryAtTableOffset(int offset) {
    IntBuffer entry = ((IntBuffer) hashtable.duplicate().position(offset).limit(offset + ENTRY_SIZE)).slice();

    if (isPresent(entry)) {
      return new DirectEntry(entry);
    } else {
      throw new AssertionError();
    }
  }

  @Override
  public Integer getSlotForHashAndEncoding(long fullHash, long encoding, long mask) {
    long maskHash = maskHash(fullHash);
    IntBuffer view = (IntBuffer) hashtable.duplicate().position(indexFor(spread(maskHash)));

    int limit = reprobeLimit();

    for (int i = 0; i < limit; i++) {
      if (!view.hasRemaining()) {
        view.rewind();
      }

      IntBuffer entry = (IntBuffer) view.slice().limit(ENTRY_SIZE);

      if (isTerminating(entry)) {
        return null;
      } else if (isPresent(entry) && (maskHash == readHash(entry)) && ((encoding & mask) == (readEncoding(entry) & mask))) {
        return view.position();
      } else {
        view.position(view.position() + ENTRY_SIZE);
      }
    }

    return null;
  }

  @Override
  public boolean updateEncoding(long fullHash, long oldEncoding, long newEncoding, long mask) {
    long maskHash = maskHash(fullHash);
    boolean updated = updateEncodingInTable(hashtable, reprobeLimit(), maskHash, oldEncoding, newEncoding, mask);

    if (hasUsedIterators) {
      pendingTableFrees.reap();
      
      Iterator<PendingPage> it = pendingTableFrees.values();
      while (it.hasNext()) {
        PendingPage pending = it.next();
        updated |= updateEncodingInTable(pending.tablePage.asIntBuffer(), pending.reprobe, maskHash, oldEncoding, newEncoding, mask);
      }
    }
    return updated;
  }

  private static boolean updateEncodingInTable(IntBuffer table, int limit, long maskHash, long oldEncoding, long newEncoding, long mask) {
    table.position(indexFor(spread(maskHash), table));

    for (int i = 0; i < limit; i++) {
      if (!table.hasRemaining()) {
        table.rewind();
      }

      IntBuffer entry = (IntBuffer) table.slice().limit(ENTRY_SIZE);

      if (isTerminating(entry)) {
        return false;
      } else if (isPresent(entry) && (maskHash == readHash(entry)) && ((oldEncoding & mask) == (readEncoding(entry) & mask))) {
        entry.put(createEntry(maskHash, (readEncoding(entry) & ~mask) | newEncoding & mask, entry.get(STATUS)));
        return true;
      } else {
        table.position(table.position() + ENTRY_SIZE);
      }
    }
    return false;
  }

  @FindbugsSuppressWarnings("VO_VOLATILE_INCREMENT")
  private void slotRemoved(IntBuffer entry) {
    modCount++;
    removedSlots++;
    size--;
    updatePendingTables(readHash(entry), readEncoding(entry), entry);
    removed(entry);
  }

  @FindbugsSuppressWarnings("VO_VOLATILE_INCREMENT")
  private void slotAdded(IntBuffer entry) {
    modCount++;
    size++;
    added(entry);
  }

  @FindbugsSuppressWarnings("VO_VOLATILE_INCREMENT")
  private void slotUpdated(IntBuffer entry, long oldEncoding) {
    modCount++;
    updatePendingTables(readHash(entry), oldEncoding, entry);
    updated(entry);
  }

  protected void added(IntBuffer entry) {
    //no-op
  }

  protected void hit(IntBuffer entry) {
    //no-op
  }

  protected void removed(IntBuffer entry) {
    //no-op
  }

  protected void updated(IntBuffer entry) {
    //no-op
  }
  
  protected void tableExpansionFailure(int start, int length) {
    StringBuilder sb = new StringBuilder("Failed to expand table.\n");
    sb.append("Current Table Size (slots) : ").append(getTableCapacity()).append('\n');
    sb.append("Resize Will Require        : ").append(DebuggingUtils.toBase2SuffixedString(getTableCapacity() * ENTRY_SIZE * (Integer.SIZE / Byte.SIZE) * 2)).append("B\n");
    sb.append("Table Buffer Source        : ").append(tableSource);
    throw new OversizeMappingException(sb.toString());
  }

  protected void storageEngineFailure(Object failure) {
    StringBuilder sb = new StringBuilder("Storage engine failed to store: ");
    sb.append(failure).append('\n');
    sb.append("StorageEngine: ").append(storageEngine);
    throw new OversizeMappingException(sb.toString());
  }

  /* MapStatistics Methods */
  @Override
  public long getSize() {
    return size;
  }

  @Override
  public long getTableCapacity() {
    IntBuffer table = hashtable;
    return table == null ? 0 : table.capacity() / ENTRY_SIZE;
  }

  @Override
  public long getUsedSlotCount() {
    return getSize();
  }

  @Override
  public long getRemovedSlotCount() {
    return removedSlots;
  }

  @Override
  public int getReprobeLength() {
    return reprobeLimit();
  }

  @Override
  public long getAllocatedMemory() {
    return getDataAllocatedMemory() + (getTableCapacity() * ENTRY_SIZE * (Integer.SIZE / Byte.SIZE));
  }

  @Override
  public long getOccupiedMemory() {
    return getDataOccupiedMemory() + (getUsedSlotCount() * ENTRY_SIZE * (Integer.SIZE / Byte.SIZE));
  }
  
  @Override
  public long getVitalMemory() {
    return getDataVitalMemory() + (getTableCapacity() * ENTRY_SIZE * (Integer.SIZE / Byte.SIZE));
  }

  @Override
  public long getDataAllocatedMemory() {
    return storageEngine.getAllocatedMemory();
  }

  @Override
  public long getDataOccupiedMemory() {
    return storageEngine.getOccupiedMemory();
  }

  @Override
  public long getDataVitalMemory() {
    return storageEngine.getVitalMemory();
  }

  @Override
  public long getDataSize() {
    return storageEngine.getDataSize();
  }

  @Override
  public boolean isThiefForTableAllocations() {
    return tableAllocationsSteal;
  }

  @Override
  public Lock readLock() {
    return NoOpLock.INSTANCE;
  }

  @Override
  public Lock writeLock() {
    return NoOpLock.INSTANCE;
  }

  public StorageEngine<? super K, ? super V> getStorageEngine() {
    return storageEngine;
  }

}
